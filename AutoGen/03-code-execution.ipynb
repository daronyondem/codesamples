{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model Configuration\n",
    "\n",
    "This cell sets up the configuration for the LLM (Language Model), defining the model details, API endpoint, and authentication. The `cache_seed` is set to `None` to disable result caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"lmstudio-community/gemma-2-2b-it-GGUF/gemma-2-2b-it-Q4_K_M.gguf:0\",\n",
    "            \"base_url\": \"http://localhost:1234/v1\",\n",
    "            \"api_key\": \"lm-studio\",\n",
    "            \"price\": [0.001, 0.001],  # Add custom pricing here (per 1k prompt/completion tokens)\n",
    "        },\n",
    "    ],\n",
    "    \"cache_seed\": None,  # Disable caching.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Modules\n",
    "\n",
    "- `Image` and `display` from `IPython.display` for displaying images in the notebook.\n",
    "- The `autogen` library, which provides the tools for creating agents and handling conversations.\n",
    "- `LocalCommandLineCodeExecutor` from `autogen.coding`, a utility that allows the assistant agent to execute local shell commands for coding tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "import autogen\n",
    "from autogen.coding import LocalCommandLineCodeExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Assistant and User Proxy Agents\n",
    "\n",
    "Setting up two agents:\n",
    "- **AssistantAgent (`assistant`)**:\n",
    "  - Uses the LLM configuration (`gemma`).\n",
    "  - Acts as the main AI assistant, capable of handling tasks and responding based on the LLM's capabilities.\n",
    "\n",
    "- **UserProxyAgent (`user_proxy`)**:\n",
    "  - Represents a user agent that interacts with the assistant.\n",
    "  - `human_input_mode` is set to `\"NEVER\"`, meaning it won't require direct user input.\n",
    "  - The agent can auto-reply up to 10 times consecutively before requiring further input or ending the task.\n",
    "  - The `is_termination_msg` lambda function checks for specific keywords (`\"TERMINATE\"` or `\"COMPLETED\"`) to decide when to stop the conversation.\n",
    "  - `code_execution_config` specifies the use of `LocalCommandLineCodeExecutor`, which allows the assistant to run shell commands in a local working directory (`coding`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AssistantAgent named \"assistant\"\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=gemma, \n",
    ")\n",
    "\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\", \"\").upper() or \"COMPLETED\" in x.get(\"content\", \"\").upper(),\n",
    "    code_execution_config={\n",
    "        # the executor to run the generated code\n",
    "        \"executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the Interaction\n",
    "\n",
    "Initiating a chat between the `user_proxy` and the `assistant`:\n",
    "- The `user_proxy` sends an initial message to the `assistant`: `\"How many r are in rasberry?\"`, unintentionally misspelling \"raspberry.\"\n",
    "- The `summary_method` is set to `\"reflection_with_llm\"`, meaning the response summary will leverage the LLM for improved reflection and analysis.\n",
    "\n",
    "The result of the conversation (`chat_res`) will contain the exchange between the two agents, including any code execution if requested by the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the assistant receives a message from the user_proxy, which contains the task description\n",
    "chat_res = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"\"\"How many r are in rasberry?\"\"\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
