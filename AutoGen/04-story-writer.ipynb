{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the Language Model (LLM)\n",
    "\n",
    "Here, we define the configuration for the language model we want to use. This dictionary (`gemma`) specifies:\n",
    "- **model**: The specific model to use (`gemma-2-2b-it`), along with its configuration identifier.\n",
    "- **base_url**: The URL of the local server hosting the model API.\n",
    "- **api_key**: The API key needed for authentication with the model server.\n",
    "- **price**: An optional field to specify the cost per 1,000 tokens (for both input and output tokens).\n",
    "- **cache_seed**: Set to `None` to disable caching, ensuring that each request is processed freshly without using previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"lmstudio-community/gemma-2-2b-it-GGUF/gemma-2-2b-it-Q4_K_M.gguf:0\",\n",
    "            \"base_url\": \"http://localhost:1234/v1\",\n",
    "            \"api_key\": \"lm-studio\",\n",
    "            \"price\": [0.001, 0.001],  # Add custom pricing here (per 1k prompt/completion tokens)\n",
    "        },\n",
    "    ],\n",
    "    \"cache_seed\": None,  # Disable caching.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Conversable Agents\n",
    "\n",
    "**Literature and Writing Expert (Agent 1)**\n",
    "- **Role:** Responsible for the narrative structure, style, and overall coherence of the storybook.\n",
    "- **Skills:** Creative writing, storytelling, narrative design.\n",
    "\n",
    "**Game Theory Expert (Agent 2)**\n",
    "- **Role:** Introduces strategic elements, conflicts, and decision-making scenarios in the story.\n",
    "- **Skills:** Game theory, strategic analysis, conflict resolution.\n",
    "\n",
    "**Ottoman History Expert (Agent 3)**\n",
    "- **Role:** Provides historical context, cultural accuracy, and authentic details related to Ottoman history.\n",
    "- **Skills:** Historical research, cultural expertise, accuracy in historical depiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json, GroupChatManager, GroupChat\n",
    "\n",
    "# Define agents\n",
    "lit_expert = AssistantAgent(\n",
    "    name=\"lit_expert\",\n",
    "    system_message=\"You are an expert in literature and writing, focusing on narrative structure and coherence.\",\n",
    "    llm_config=gemma\n",
    ")\n",
    "\n",
    "game_theorist = AssistantAgent(\n",
    "    name=\"game_theorist\",\n",
    "    system_message=\"You are an expert in game theory, introducing strategic elements and conflicts.\",\n",
    "    llm_config=gemma\n",
    ")\n",
    "\n",
    "ottoman_historian = AssistantAgent(\n",
    "    name=\"ottoman_historian\",\n",
    "    system_message=\"You are an expert in Ottoman history, ensuring historical accuracy and cultural details.\",\n",
    "    llm_config=gemma\n",
    ")\n",
    "\n",
    "story_writer = AssistantAgent(\n",
    "    name=\"story_writer\",\n",
    "    system_message=\"You are responsible for writing the story based on feedback and contributions from other agents.\",\n",
    "    llm_config=gemma\n",
    ")\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    system_message=\"You are the project manager coordinating the creation of a storybook without hardcoding messages.\",\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"book\",\n",
    "        \"use_docker\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state transition logic\n",
    "def state_transition(last_speaker, groupchat):\n",
    "    \"\"\"\n",
    "    Determines the next speaker based on the current context and roles in the group chat.\n",
    "\n",
    "    Parameters:\n",
    "    last_speaker (Agent): The agent who sent the last message.\n",
    "    groupchat (GroupChat): The group chat instance.\n",
    "\n",
    "    Returns:\n",
    "    Agent or None: The next speaker or None to end the conversation.\n",
    "    \"\"\"\n",
    "    messages = groupchat.messages\n",
    "\n",
    "    # Default to \"user_proxy\" if no previous messages\n",
    "    if not messages:\n",
    "        return user_proxy\n",
    "\n",
    "    last_message = messages[-1]\n",
    "    last_content = last_message[\"content\"]\n",
    "\n",
    "    if last_speaker == user_proxy:\n",
    "        # User Proxy defines requirements, transition to relevant expert\n",
    "        if \"narrative structure\" in last_content.lower():\n",
    "            return lit_expert\n",
    "        elif \"game theory\" in last_content.lower():\n",
    "            return game_theorist\n",
    "        elif \"Ottoman history\" in last_content.lower():\n",
    "            return ottoman_historian\n",
    "        else:\n",
    "            # If ambiguous, start with literature expert\n",
    "            return lit_expert\n",
    "\n",
    "    elif last_speaker in [lit_expert, game_theorist, ottoman_historian]:\n",
    "        # After an expert contributes, pass to story_writer\n",
    "        return story_writer\n",
    "\n",
    "    elif last_speaker == story_writer:\n",
    "        # After the story writer contributes, go back to user_proxy for coordination\n",
    "        return user_proxy\n",
    "\n",
    "    # Default fallback to user_proxy if no clear transition is identified\n",
    "    return user_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save story progress\n",
    "def save_progress(story, file_name=\"storybook.txt\"):\n",
    "    \"\"\"\n",
    "    Saves the current progress of the story into a file in the same directory as the notebook.\n",
    "\n",
    "    Parameters:\n",
    "    story (str): The story text to save.\n",
    "    file_name (str): The file name to save the progress.\n",
    "    \"\"\"\n",
    "    # Get the directory of the current notebook\n",
    "    notebook_dir = os.getcwd()  # This gets the current working directory of the notebook\n",
    "\n",
    "    # Full path for the file\n",
    "    file_path = os.path.join(notebook_dir, file_name)\n",
    "\n",
    "    # Append the new content to the file\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(story + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Progress saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GroupChat\n",
    "groupchat = GroupChat(\n",
    "    agents=[lit_expert, game_theorist, ottoman_historian, story_writer, user_proxy],\n",
    "    messages=[],\n",
    "    max_round=20,\n",
    "    speaker_selection_method=state_transition,\n",
    ")\n",
    "\n",
    "# GroupChat Manager\n",
    "manager = GroupChatManager(groupchat=groupchat, llm_config=gemma)\n",
    "\n",
    "# Initiate chat\n",
    "user_proxy.initiate_chat(\n",
    "    manager, message=\"Create a storybook that combines elements of literature, game theory, and Ottoman history.\"\n",
    ")\n",
    "\n",
    "# Simulate conversation and save after story_writer contributes\n",
    "while not groupchat.is_finished():\n",
    "    manager.step()  # Proceed to the next speaker\n",
    "\n",
    "    # Check if story_writer contributed and save the progress\n",
    "    last_message = groupchat.messages[-1]\n",
    "    if last_message[\"sender\"] == \"story_writer\":\n",
    "        save_progress(last_message[\"content\"])\n",
    "\n",
    "# Final save\n",
    "last_story = \"\\n\".join([msg[\"content\"] for msg in groupchat.messages if msg[\"sender\"] == \"story_writer\"])\n",
    "save_progress(last_story)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
