{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain LLM Wrappers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard LLM wrappers for LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "llm = AzureOpenAI(model_kwargs={'engine':'text-davinci-003'})\n",
    "llm(\"Hello. I'm Daron!\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switching to the Chat Model in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AzureChatOpenAI(\n",
    "    openai_api_base=os.environ.get(\"OPENAI_API_BASE\"),\n",
    "    openai_api_version=os.environ.get(\"OPENAI_CHAT_API_VERSION\"),\n",
    "    deployment_name=os.environ.get(\"OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "    openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    openai_api_type=os.environ.get(\"OPENAI_API_TYPE\"),\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert cloud architect in love with Azure\"),\n",
    "    HumanMessage(content=\"Tell me what NoSQL Service is the best for a global deployment?\"),\n",
    "]\n",
    "response = chat(messages) \n",
    "print(response.content, end='\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are an expert cloud architect in love with Azure. Explain to me the benefits of {concept}.\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"concept\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(prompt.format(concept=\"Azure Functions\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain                 \n",
    "\n",
    "second_template = \"\"\"You are an expert educator and trainer. Help the below content making it easy to understand for anyone. {newconcept}\"\"\"\n",
    "second_prompt = PromptTemplate(\n",
    "    template=second_template,\n",
    "    input_variables=[\"newconcept\"]\n",
    "    )\n",
    "\n",
    "explainerChain = LLMChain(llm=llm, prompt=prompt)\n",
    "educatorChain = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "learningChain = SimpleSequentialChain(chains=[explainerChain, educatorChain], verbose=True)\n",
    "\n",
    "output = learningChain.run(\"Azure Functions\")\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading local text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "file_name = \"azure-functions-June-2023-Updates.txt\"\n",
    "file_path = os.path.join(current_directory, file_name)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as file:\n",
    "    contentUpdate = file.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting text into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 0,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([contentUpdate])\n",
    "\n",
    "print(texts[0].page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "import uuid\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    response = openai.Embedding.create(\n",
    "        input=text, engine=\"text-embedding-ada-002\")\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings\n",
    "\n",
    "embedding_collection = []\n",
    "for chunk in texts:\n",
    "    embedding_collection.append({\n",
    "        'id': str(uuid.uuid4()),  # Random UUID as ID\n",
    "        'content': chunk.page_content,  # the title of the page\n",
    "        'contentVector': generate_embeddings(chunk.page_content)  # the embedding data\n",
    "    })\n",
    "\n",
    "print(embedding_collection[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorStores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing Dev Package for Azure Cognitive Search as the Vector Search feature is still in preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-search-documents==11.4.0a20230509004 --index-url https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/ -vvv --no-deps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential \n",
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from azure.search.documents.indexes.models import (  \n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SearchableField,\n",
    "    SimpleField,  \n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    VectorSearch,  \n",
    "    VectorSearchAlgorithmConfiguration,  \n",
    ")   \n",
    "\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")  \n",
    "credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_ADMIN_KEY\"))\n",
    "\n",
    "index_client = SearchIndexClient(endpoint=service_endpoint, credential=credential)\n",
    "\n",
    "#Azure OpenAI model, text-embedding-ada-002 with 1,536 dimensions means one document would consume 1,536 floats.\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String,\n",
    "                    searchable=True, retrievable=True),\n",
    "    SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, dimensions=1536, vector_search_configuration=\"my-vector-config\"),\n",
    "]\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    algorithm_configurations=[\n",
    "        VectorSearchAlgorithmConfiguration(\n",
    "            name=\"my-vector-config\",\n",
    "            kind=\"hnsw\", # HNSW is the only supported algorithm for now (Hierarchical Navigable Small World)\n",
    "            hnsw_parameters={\n",
    "                \"m\": 4, # the number of bi-directional links created for every new element during construction\n",
    "                \"efConstruction\": 400, # the maximum number of nodes to visit during the construction of the graph\n",
    "                \"efSearch\": 1000, # the maximum number of nodes to visit during the search\n",
    "                \"metric\": \"cosine\"\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "index = SearchIndex(name=\"vectorindex\", fields=fields,\n",
    "                    vector_search=vector_search)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedd uploads to Azure Cognitive Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from azure.search.documents import SearchClient \n",
    "\n",
    "search_client = SearchClient(endpoint=service_endpoint, index_name='vectorindex', credential=credential)\n",
    "result = search_client.upload_documents(embedding_collection)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing a vector similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you provide the timestamp for the most recent information you have on Azure Functions? Please specify the date and time up to your last update.\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import Vector \n",
    "from itertools import tee\n",
    "  \n",
    "search_client = SearchClient(service_endpoint, \"vectorindex\", AzureKeyCredential(os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")))  \n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=\"\",  \n",
    "    vector=Vector(value=generate_embeddings(query), k=3, fields=\"contentVector\"),  \n",
    "    select=[\"id\", \"content\"] \n",
    ")  \n",
    "\n",
    "# Initialize a string to store the results\n",
    "top_results = \"\"\n",
    "\n",
    "results, results_backup = tee(results)\n",
    "  \n",
    "for result in results_backup:  \n",
    "    top_results += f\"Score: {result['@search.score']}\\nContent: {result['content']}\\n\\n\"\n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrievel Augmented Generation (RAG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard GPT Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"Can you provide the timestamp for the most recent information you have on Azure Functions? Please specify the date and time up to your last update.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmented GPT Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "rag_template = '''\n",
    "###\n",
    "CONTEXT:\n",
    "\"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "###\n",
    "RULES:\n",
    "You are a helpful agent that answers questions.\n",
    "Learn from the CONTEXT and answer the following question.\n",
    "###\n",
    "{question}\n",
    "'''\n",
    "rag_prompt = PromptTemplate(\n",
    "        template=rag_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "llm(rag_prompt.format(question=\"Can you provide the timestamp for the most recent information you have on Azure Functions? Please specify the date and time up to your last update.\", context=top_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
