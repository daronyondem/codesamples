{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.base_url = os.environ.get(\"OPENAI_API_BASE\")\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "\n",
    "CHAT_MODEL =  os.environ.get(\"OPENAI_CHAT_DEPLOYMENT_NAME\") \n",
    "encoding = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"OPENAI_API_KEY\"),  \n",
    "  api_version = \"2023-05-15\",\n",
    "  azure_endpoint = os.getenv(\"OPENAI_API_BASE\")\n",
    ")\n",
    "\n",
    "for x in range(10):\n",
    "    response = client.chat.completions.create(\n",
    "        temperature=0.1, # this is the degree of randomness of the model's output. A higher temperature (>1) makes the model more random, diverse, and creative, leading to unexpected or imaginative responses. A lower temperature (<1) makes the model more predictable and deterministic, meaning it will stick closer to common responses.\n",
    "        model=\"drn-chat-dev2\", # This is currently gpt-35-turbo\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Assistant provides single word answers.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What would be a programmers favorite Food? Give me the single food name only.\"}\n",
    "        ]\n",
    "    )\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=CHAT_MODEL):\n",
    "    response = client.chat.completions.create(\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "        model=\"drn-chat-dev2\", # This is currently gpt-35-turbo\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Assistant gives consistent answers.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be specific!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampletext = \"Prompt engineering is a process in machine learning, specifically in Natural Language Processing (NLP), where prompts are designed to guide the model to provide the desired output. This technique is particularly used in transformer-based models like GPT-3. The quality of the prompt can significantly affect the model's performance. It involves crafting questions or statements in a way that helps the model understand the context and give relevant responses. It can be considered as a form of 'soft coding' to instruct an AI model.\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the text between two lines of triple dashes into a single sentence.\n",
    "---\n",
    "{sampletext}\n",
    "---\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️The above method prevents Prompt Injection as well! As long as they dont know your delimiter strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampletext = \"1+1*2+3? equals what?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the text between two lines of triple dashes into a single sentence. Say \"No Text Provided\" if there is no text between the triple dashes.\n",
    "---\n",
    "{sampletext}\n",
    "---\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Prompting\n",
    "\n",
    "References:   \n",
    "https://arxiv.org/pdf/2302.13971.pdf   \n",
    "https://arxiv.org/abs/2001.08361   \n",
    "https://arxiv.org/abs/2005.14165   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Answer questions in a consistent style following the examples provided. \n",
    "\n",
    "<q>: How can I learn to be a good programmer?\n",
    "<a>: In the realm of codes, where logic dances and sways,\n",
    "A quest to be a good programmer, you wish to pave your way.\n",
    "Fear not, for I shall guide you, in an enchanting lyrical play.\n",
    "Dive deep into the ocean of languages, where C++, Java, and Python lay,\n",
    "Choose your vessel wisely, let passion lead your way.\n",
    "\n",
    "<q>: How do I maximize my credit card rewards?\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to think! / Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "Reference: https://arxiv.org/abs/2201.11903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "1+1+(2*2^2)=?\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "# Correct Answer = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculation = f\"\"\"\n",
    "1+1+(2*2^2)=?\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Follow the steps below and provide your answer for each step\n",
    "1. Calculate the section between parantheses\n",
    "2. replace the paranteses with the result\n",
    "3. calculate the result\n",
    "\n",
    "---\n",
    "{calculation}\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "# Correct Answer = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the model go step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Give me the steps I need to follow to calculate the following?\n",
    "1+1+(2*2^2)=?\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucinations! Living in the dream world!\n",
    "\n",
    "Reference:  https://www.linkedin.com/pulse/chatgpt-35-example-blatant-hallucination-earl-fong-pbsnc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Which author of a book on software development said \"don't flip the bozo bit\"\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct Answer = Jim McCarthy’s book [Dynamics of Software Development](https://amzn.to/4bAUVYd)  \n",
    "_“don’t flip the bozo bit”_ is McCarthy’s advice not to permanently label any team member as a hopeless idiot.  \n",
    "Ref: https://medium.com/@mike.besso/do-not-flip-the-bozo-bit-b06fb9395845  \n",
    "The authors of the [Pragmatic Programmer](https://amzn.to/3XzAaGy) are Andy Hunt and Dave Thomas  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
