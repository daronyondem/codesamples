{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "CHAT_MODEL =  os.environ.get(\"OPENAI_CHAT_DEPLOYMENT_NAME\") \n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"OPENAI_API_KEY\"),  \n",
    "  api_version = \"2023-05-15\",\n",
    "  azure_endpoint = os.getenv(\"OPENAI_API_BASE\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=CHAT_MODEL):\n",
    "    response = client.chat.completions.create(\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "        model=\"drn-chat-dev2\", # This is currently gpt-35-turbo\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Consistency\n",
    "\n",
    "The primary objective is to improve the simplistic greedy decoding often used in chain-of-thought (CoT) prompting. In this technique, several diverse reasoning paths are sampled through few-shot CoT. The responses generated from these paths are then evaluated, and the most consistent answer is selected. This method has a significant impact on enhancing the effectiveness of CoT prompting, particularly in tasks that require arithmetic and commonsense reasoning. \n",
    "\n",
    "Reference: https://arxiv.org/abs/2203.11171\n",
    "\n",
    "![Self-Consistency](./images/self-consistency-cot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "When I was 12 my sister was half my age. Now\n",
    "I’m 70 how old is my sister?\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
    "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
    "The answer is 29.\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
    "golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
    "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent $15. She has $8 left.\n",
    "Q: When I was 12 my sister was half my age. Now I’m 70 how old is my sister?\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Knowledge Prompting\n",
    "\n",
    "Reference: https://arxiv.org/pdf/2110.08387.pdf\n",
    "\n",
    "![](./images/generated-knowledge-prompting.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Which country is larger, Congo or South Africa?\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Generate some knowledge about the sizes of South Africa and Congo:\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "South Africa is a country located in the southernmost part of the African continent. It has a total land area of approximately 1,219,090 square kilometers, making it the 25th largest country in the world. South Africa is bordered by Namibia, Botswana, Zimbabwe, Mozambique, and Swaziland, and it has a coastline that stretches for 2,798 kilometers along the Atlantic and Indian Oceans.\n",
    "\n",
    "On the other hand, the Democratic Republic of Congo, also known as Congo-Kinshasa, is a country located in Central Africa. It has a total land area of approximately 2,344,858 square kilometers, making it the 11th largest country in the world. Congo is bordered by nine countries, including Angola, Zambia, Tanzania, and Uganda, and it has a coastline that stretches for 37 kilometers along the Atlantic Ocean.\n",
    "\n",
    "In terms of size, Congo is almost twice as large as South Africa. However, South Africa has a higher population density, with an estimated population of 59.3 million people compared to Congo's estimated population of 89.6 million people.\n",
    "\n",
    "Which country is larger, Congo or South Africa?\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree of Thoughts (ToT)\n",
    "\n",
    "References:\n",
    "https://arxiv.org/abs/2305.10601\n",
    "https://arxiv.org/abs/2305.08291"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree of Thoughts (ToT)\n",
    "\n",
    "![](./images/tot.png)\n",
    "\n",
    "References:   \n",
    "https://arxiv.org/abs/2305.10601     \n",
    "https://arxiv.org/abs/2305.08291   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Imagine five different experts are answering this question.\n",
    "All experts will write down 1 step of their thinking,\n",
    "then share it with the group.\n",
    "Then all experts will go on to the next step, etc.\n",
    "If any expert realises they're wrong at any point then they leave.\n",
    "The question is...\n",
    "\n",
    "Which country is larger, Congo or South Africa? \n",
    "\n",
    "Give me a final conclusion.\n",
    "\"\"\"\n",
    "#Prompt reference: https://github.com/dave1010/tree-of-thought-prompting\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rephase and Respond (RaR)\n",
    "\n",
    "References:\n",
    "https://arxiv.org/abs/2311.04205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Paul is born on 20th of January 1990. Was Paul born in an even day?\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Paul is born on 20th of January 1990. Was Paul born in an even day?\n",
    "\n",
    "Rephrase and expand the question, and respond.\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrievel Augmented Generation\n",
    "\n",
    "![](./images/rag.jpg)\n",
    "\n",
    "Reference: https://arxiv.org/pdf/2005.11401.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotional Prompting!\n",
    "\n",
    "\n",
    "\n",
    "![](./images/tipping-llm.png)\n",
    "\n",
    "References: \n",
    "https://x.com/voooooogel/status/1730726744314069190   \n",
    "Large Language Models Understand and Can be Enhanced by Emotional Stimuli - https://arxiv.org/abs/2307.11760 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-qna-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4ee1bbf3137c7ea9420c4fd488a55642063e5739fe2a7286130d9ba47405b69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
