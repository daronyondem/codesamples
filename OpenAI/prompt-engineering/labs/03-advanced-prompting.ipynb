{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b822ec",
   "metadata": {},
   "source": [
    "## Advanced Prompting Patterns\n",
    "\n",
    "Welcome to the hands-on lab for **Module 3: Advanced Prompting Patterns**. \n",
    "\n",
    "In this notebook, we'll walk through practical examples of:  \n",
    "2. **Chain of Thought (CoT)** prompting  \n",
    "3. **Tree of Thoughts (ToT)**  \n",
    "4. **Algorithm of Thoughts**  \n",
    "5. **Generated Knowledge** prompting  \n",
    "6. **Rephrase and Respond (RaR)**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbde50d",
   "metadata": {},
   "source": [
    "### 1. Setup\n",
    "\n",
    "Let's define a helper function to send chat prompts to the model for convenience. This function will call the `/v1/chat/completions` endpoint with the given messages and parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0be8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Replace with the correct endpoint for your local LLM server\n",
    "LLM_API_URL = \"http://localhost:1234/v1/chat/completions\"\n",
    "\n",
    "# A helper function to send chat-based requests to the locally hosted openchat-3.5-0106 model\n",
    "def generate_response(\n",
    "    messages, \n",
    "    temperature=0.7, \n",
    "    max_tokens=256, \n",
    "    top_p=1.0, \n",
    "    n=1, \n",
    "    stream=False\n",
    "):\n",
    "    payload = {\n",
    "        \"model\": \"openchat-3.5-0106\",\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"n\": n,\n",
    "        \"stream\": stream\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(LLM_API_URL, headers=headers, json=payload)\n",
    "    \n",
    "    # The structure of the response may vary by implementation.\n",
    "    # Adjust parsing logic if needed.\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # For a ChatCompletion, responses are typically found in data[\"choices\"][i][\"message\"][\"content\"]\n",
    "        # We'll return the first choice's content for simplicity.\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    else:\n",
    "        raise Exception(f\"Request failed with status code {response.status_code} and message: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272845f6",
   "metadata": {},
   "source": [
    "### 2. Chain of Thought (CoT) Prompting\n",
    "\n",
    "Chain of Thought prompting encourages the model to reason step-by-step. One common pattern is to explicitly instruct the model to show or use its reasoning (though for production systems, you might keep the chain-of-thought hidden).\n",
    "\n",
    "#### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf1106",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a helpful assistant. \"\n",
    "            \"Use detailed reasoning to arrive at the correct answer, but provide a concise final answer.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"A coffee shop sells a latte for $4.00. If they sell 150 lattes in a day, \"\n",
    "            \"how much revenue do they make from lattes?\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            # We request the chain-of-thought reasoning.\n",
    "            \"Let's break down the problem step by step. Then provide a short final answer at the end.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "response = generate_response(messages, temperature=0.5)\n",
    "print(\"Chain-of-Thought Response:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df9bda",
   "metadata": {},
   "source": [
    "**Exercise:**  \n",
    "- Try adjusting the `messages` to explicitly ask for “show your reasoning” vs. “think through the reasoning but do NOT show it.” Observe the difference in the model’s behavior.\n",
    "- Experiment with `temperature` to see how the detail or style changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85098ae2",
   "metadata": {},
   "source": [
    "### 3. Tree of Thoughts (ToT)\n",
    "\n",
    "Tree of Thoughts prompting is an approach that encourages the model to branch out multiple possible reasoning paths before converging on a final solution.\n",
    "\n",
    "**Note:** Implementing ToT often involves multiple calls to the model, manually orchestrating the \"branching\" and \"exploration.\" Below is a simplified version where we encourage the model to outline multiple solution paths in one call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817a2624",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a problem-solving assistant. \"\n",
    "            \"When given a question, explore multiple possible solution paths (like branching thoughts), \"\n",
    "            \"evaluate them, and then converge to the best answer.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"I want to invest in the stock market. Outline a few different strategies I could use, \"\n",
    "            \"then decide which strategy might work best for a moderate risk profile.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "response = generate_response(messages, temperature=0.7, max_tokens=400)\n",
    "print(\"Tree of Thoughts Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09601f2d",
   "metadata": {},
   "source": [
    "**Exercise:**  \n",
    "- Modify the user prompt to be more specific (e.g., “for technology stocks” or “in a bullish market”).  \n",
    "- Increase the `max_tokens` if the model gets cut off while enumerating multiple paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad1033c",
   "metadata": {},
   "source": [
    "### 4. Algorithm of Thoughts\n",
    "\n",
    "Algorithm of Thoughts is a prompting technique where you ask the model to apply a systematic, step-by-step procedure to solve a problem. It’s similar to CoT but emphasizes a strict algorithmic approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0ed18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an assistant that solves problems using a very systematic approach. \"\n",
    "            \"Always outline a clear algorithm or checklist before arriving at a conclusion.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Determine if the string 'racecar' is a palindrome using a step-by-step checklist.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "response = generate_response(messages, temperature=0.3)\n",
    "print(\"Algorithm of Thoughts Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8bc070",
   "metadata": {},
   "source": [
    "**Exercise:**  \n",
    "- Adapt this approach to a real-world scenario: “Find prime factors of 84 using a systematic approach.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a130f47",
   "metadata": {},
   "source": [
    "### 5. Generated Knowledge Prompting\n",
    "\n",
    "Generated Knowledge prompting is a technique where you have the model generate knowledge or context first, and then feed that context back in a subsequent prompt. This helps separate factual recall/generation from the final answer construction.\n",
    "\n",
    "#### Example Flow\n",
    "\n",
    "1. Ask the model to generate background knowledge on a topic.  \n",
    "2. Provide that knowledge as context to a second prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f26c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate knowledge\n",
    "messages_knowledge = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a research assistant who provides concise background information.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain the concept of dynamic programming in simple terms.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "background_info = generate_response(messages_knowledge, temperature=0.7)\n",
    "print(\"Background Info:\\n\", background_info)\n",
    "\n",
    "# Step 2: Use that knowledge in a new query\n",
    "messages_final = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "                \"You are a teaching assistant. Use the background information provided by the user \"\n",
    "                \"to craft your final answer.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            f\"Background info:\\n{background_info}\\n\\n\"\n",
    "            \"Now, can you give me a real-world example demonstrating dynamic programming?\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "final_response = generate_response(messages_final, temperature=0.7)\n",
    "print(\"\\nFinal Answer (Informed by Generated Knowledge):\\n\", final_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ed0ba",
   "metadata": {},
   "source": [
    "**Exercise:**  \n",
    "- Try this technique for other topics (e.g., “quantum computing,” “k-means clustering,” etc.).  \n",
    "- Compare how well the second answer does when you feed it the generated knowledge vs. not providing that context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d620ed29",
   "metadata": {},
   "source": [
    "### 6. Rephrase and Respond (RaR)\n",
    "\n",
    "With RaR, you first ask the model to restate the user’s question in its own words (often improving clarity), then provide an answer. This can reduce ambiguity.\n",
    "\n",
    "#### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f13e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a helpful assistant. First rephrase the user's question, \"\n",
    "            \"then provide your best answer.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"What's the best approach to manage microservices in a large enterprise?\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "response = generate_response(messages, temperature=0.7)\n",
    "print(\"Rephrase and Respond:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee38e0",
   "metadata": {},
   "source": [
    "**Exercise:**  \n",
    "- Modify the user question to be longer or more ambiguous, then see if rephrasing clarifies it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7add128",
   "metadata": {},
   "source": [
    "### 7. Exercises and Experiments\n",
    "\n",
    "Now that you’ve walked through each pattern, here are a few suggestions to deepen your understanding:\n",
    "\n",
    "1. **Combine Patterns**  \n",
    "   - Use **Generated Knowledge** to build a context or scenario, and then apply **Chain of Thought** to reason about that scenario.\n",
    "\n",
    "2. **Experiment with Self-Consistency**  \n",
    "   - Use the **Tree of Thoughts** approach to generate multiple branches, then apply the self-consistency technique to see which branch is most robust.\n",
    "\n",
    "3. **Benchmarking**  \n",
    "   - For each technique, measure response times and token usage to understand cost/performance implications.\n",
    "\n",
    "4. **Extend the Notebook**  \n",
    "   - Add additional cells where you create your own advanced prompts using any combination of the patterns above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a696e9",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this notebook, you have:\n",
    "- Explored **Advanced Prompting Patterns** like CoT, ToT, Algorithm of Thoughts, Generated Knowledge, and RaR.\n",
    "- Practiced how to structure prompts and parse model responses.\n",
    "- Seen how to orchestrate multiple calls for advanced techniques (like self-consistency voting or multi-step knowledge generation).\n",
    "\n",
    "Feel free to continue experimenting with these patterns to see how they can be combined or adapted to fit real-world development scenarios. You now have hands-on experience building and testing advanced prompt designs for large language models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompten",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
