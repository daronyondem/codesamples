{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with LM Studio and Prompt Calls\n",
    "\n",
    "**Overview:** In this hands-on lab, you will go through the steps of setting up your local LLM environment and making some initial prompt calls. The lab is structured as a Jupyter notebook, with explanatory text and code cells. You‚Äôll get to practice adjusting parameters like temperature and max tokens, and see how prompt formatting (system/user messages) affects the output. This lab is aimed at mid-level developers, so we assume you‚Äôre comfortable running Python code and making HTTP requests. \n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "- Connect to the local LM Studio server from code.\n",
    "- Send both completion and chat requests to the OpenChat-3.5 model.\n",
    "- Experiment with different generation settings and observe their effects.\n",
    "- Understand how token limits can impact your prompts and outputs.\n",
    "\n",
    "Let‚Äôs get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Verify the Local LLM Server**\n",
    "\n",
    "Before sending complex prompts, do a quick check that we can reach the LM Studio server and that our model is listed.\n",
    "\n",
    "**Exercise 1.1: List models via API.** Run the following cell to list the models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "BASE_URL = \"http://localhost:1234\"  # LM Studio's default API base URL\n",
    "\n",
    "res = requests.get(f\"{BASE_URL}/v1/models\")\n",
    "if res.status_code == 200:\n",
    "    models = res.json()\n",
    "    print(\"Available models:\", models)\n",
    "else:\n",
    "    print(\"Could not fetch models. Is LM Studio running and the server enabled?\")\n",
    "    print(\"Status code:\", res.status_code, res.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Expected Result:** You should see a JSON response containing at least one model. Look for the `\"id\"` field ‚Äì that is the model‚Äôs identifier. It might be something like `\"openchat-3.5-0106\"` or similar. If you see an empty list or an error, double-check that LM Studio‚Äôs local server is on (in the app‚Äôs settings) and the port matches.\n",
    "\n",
    "**Question:** *If no models are listed or you got an error, how would you troubleshoot?*  \n",
    "*(Hint: Check LM Studio is running the server on the expected port. You can also try `curl http://localhost:1234/v1/health` (if available) or ensure your firewall isn‚Äôt blocking local calls.)*\n",
    "\n",
    "Once the model is confirmed, we can proceed to querying it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Your First Completion Call**\n",
    "\n",
    "Now, let‚Äôs send a simple prompt to the model to make sure everything is working end-to-end.\n",
    "\n",
    "**Exercise 2.1: Make a completion request.** We‚Äôll ask the model a basic question: *‚ÄúWhat is prompt engineering?‚Äù* This will use the `/v1/completions` endpoint, treating the query as a single-turn prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"model\": \"openchat-3.5-0106\",  # replace with your model ID if different\n",
    "    \"prompt\": \"What is prompt engineering?\",\n",
    "    \"max_tokens\": 100,   # expecting a short paragraph answer\n",
    "    \"temperature\": 0.5   # moderate temperature for a factual answer\n",
    "}\n",
    "response = requests.post(f\"{BASE_URL}/v1/completions\", json=payload)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    answer = data.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
    "    print(\"Model's answer:\\n\", answer.strip())\n",
    "else:\n",
    "    print(\"Request failed:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Run the cell** and observe the output. You should get a coherent explanation of what prompt engineering is. If the answer is cut off or seems incomplete, check if `max_tokens` might have been too low. If it‚Äôs too verbose or wandering, note that for later when we adjust temperature.\n",
    "- This is your ‚ÄúHello, world‚Äù with the LLM. You sent a prompt and got a completion back. üéâ\n",
    "\n",
    "**Think About:** The model likely gave an answer defining prompt engineering (e.g. ‚Äúcrafting effective inputs to guide AI models to produce desired outputs‚Äù). Does it align with your understanding? This is a good sanity check that the model knows the basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Chat API ‚Äì Structured Conversation**\n",
    "\n",
    "Next, let‚Äôs use the chat format, which is more powerful for interactive prompts or setting context via roles.\n",
    "\n",
    "**Exercise 3.1: Use a system message to influence style.** We‚Äôll ask the same question, but this time instruct the model (via a system role) to respond in a particular style ‚Äì say, a formal academic tone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant's answer (formal style):\n",
      " Prompt engineering refers to the process of designing, developing, and optimizing input prompts for AI language models such as GPT-3 or GPT-4 in order to generate more accurate, relevant, and useful responses. This involves understanding the capabilities and limitations of the model, as well as the specific context and goals of the user's query. By carefully crafting prompts, prompt engineers can guide the AI to produce higher quality outputs and improve overall user experience. Techniques used\n"
     ]
    }
   ],
   "source": [
    "chat_payload = {\n",
    "    \"model\": \"openchat-3.5-0106\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise and formal academic assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is prompt engineering?\"}\n",
    "    ],\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_tokens\": 100\n",
    "}\n",
    "chat_res = requests.post(f\"{BASE_URL}/v1/chat/completions\", json=chat_payload)\n",
    "answer = None\n",
    "if chat_res.status_code == 200:\n",
    "    chat_data = chat_res.json()\n",
    "    answer = chat_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "    print(\"Assistant's answer (formal style):\\n\", answer.strip())\n",
    "else:\n",
    "    print(\"Chat request failed:\", chat_res.status_code, chat_res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Observe the style/tone:** The content of the answer should still explain prompt engineering, but perhaps the wording is more formal or succinct (because we set the persona to ‚Äúconcise and formal academic‚Äù). If you remove or change the system message to something like ‚ÄúYou are a casual, friendly assistant‚Äù and run again, you‚Äôd likely see a difference in tone. Feel free to try that as an experiment!\n",
    "\n",
    "**Exercise 4.2: Multi-turn conversation.** Let‚Äôs extend the conversation. Imagine we follow up with another question. In a chat scenario, we need to include the conversation history in the next request (since the API is stateless). We‚Äôll append a new user message and send the whole message list again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant's follow-up answer:\n",
      " Prompt engineering is crucial for AI developers because it significantly impacts the performance, efficiency, and effectiveness of AI language models in addressing user queries or tasks. The importance lies in several aspects:\n",
      "\n",
      "1. Enhanced Accuracy: Well-designed prompts help AI models generate more accurate responses by providing clear and concise information, reducing ambiguity and misinterpretation.\n",
      "2. Contextual Relevance: Prompt engineering ensures that the AI model understands the context of the\n"
     ]
    }
   ],
   "source": [
    "# Continue the conversation with a follow-up question\n",
    "if answer:\n",
    "    followup_messages = chat_payload[\"messages\"] + [\n",
    "        {\"role\": \"assistant\", \"content\": answer},\n",
    "        {\"role\": \"user\", \"content\": \"Why is it important for AI developers?\"}\n",
    "    ]\n",
    "    follow_payload = {\n",
    "        \"model\": \"openchat-3.5-0106\",\n",
    "        \"messages\": followup_messages,\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    "    follow_res = requests.post(f\"{BASE_URL}/v1/chat/completions\", json=follow_payload)\n",
    "    if follow_res.status_code == 200:\n",
    "        follow_data = follow_res.json()\n",
    "        follow_answer = follow_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "        print(\"Assistant's follow-up answer:\\n\", follow_answer.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we:\n",
    "- Took the original messages (system + first user question),\n",
    "- Added the assistant‚Äôs answer as a message with role `\"assistant\"`,\n",
    "- Then a new user question *‚ÄúWhy is it important for AI developers?‚Äù*,\n",
    "- Sent the whole sequence again to get a follow-up answer.\n",
    "\n",
    "- **Run the above cell** and see the response. The assistant‚Äôs answer should reference the context (it knows we were talking about prompt engineering) and explain why it‚Äôs important, perhaps building on the prior answer. This shows how the model uses the conversation history. It‚Äôs remembering what was said (within the context window).\n",
    "\n",
    "This multi-turn approach is essential when you want the model to have a back-and-forth dialogue, or when you want to feed the model its own previous answer (for verification or refining further).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Playing with Temperature**\n",
    "\n",
    "Now that basic Q&A is working, let‚Äôs systematically explore the effect of `temperature`. \n",
    "\n",
    "**Exercise 45.1: High vs Low Temperature Output.** We‚Äôll use a creative prompt to see how the style changes. Run the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temperature 0.2:\n",
      "The sun was setting over the horizon, casting an orange glow across the futuristic city skyline. The towering buildings, adorned with neon lights and holographic advertisements, stretched up into the sky like a forest of glass and steel. As the sun\n",
      "\n",
      "\n",
      "Temperature 0.9:\n",
      "It was the first time that I had seen anything like it, and it took my breath away. The sky was a brilliant swirl of reds, yellows, pinks, purples, and oranges, creating a spectacular scene that would be ingrained in my\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"Describe a sunset over a futuristic city skyline.\"\n",
    "for temp in [0.2, 0.9]:\n",
    "    payl = {\n",
    "        \"model\": \"openchat-3.5-0106\",\n",
    "        \"prompt\": test_prompt,\n",
    "        \"max_tokens\": 60,\n",
    "        \"temperature\": temp\n",
    "    }\n",
    "    resp = requests.post(f\"{BASE_URL}/v1/completions\", json=payl).json()\n",
    "    text = resp.get(\"choices\", [{}])[0].get(\"text\", \"\").strip()\n",
    "    print(f\"\\nTemperature {temp}:\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will prompt the model to *‚ÄúDescribe a sunset over a futuristic city skyline.‚Äù* once with temperature 0.2 (quite low) and once with 0.9 (high).\n",
    "\n",
    "- **Compare the outputs:** \n",
    "  - At **0.2**, the description might be straightforward, perhaps focusing on clear details like ‚ÄúThe sun sets, casting orange hues on glass skyscrapers... It is calm and peaceful.‚Äù\n",
    "  - At **0.9**, the description might become more vivid, imaginative or unexpected, maybe introducing creative elements like ‚Äúneon lights flicker on as the sky burns red-orange, reflecting off hovering vehicles...‚Äù.\n",
    "  \n",
    "Are the two descriptions different in phrasing or imagery? Note these differences. This reinforces how temperature can make the model either play it safe or go wild with the description.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.2: When to use what?** Consider a scenario: You‚Äôre building a factual Q&A bot versus a poetry generator. Which temperature setting would you choose for each and why? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5: Understanding Token Limits**\n",
    "\n",
    "Finally, let‚Äôs ensure we understand token limits practically. Our model has a context window of 8192 tokens. It‚Äôs hard to hit that in a quick test without a huge input, but we can simulate the effect of smaller token limits using `max_tokens` (for output) and by observing input lengths.\n",
    "\n",
    "**Exercise 5.1: Long Prompt Truncation (Thought Experiment).** We won‚Äôt actually use an 8000-token prompt here, but consider: if you feed the model input that exceeds 8192 tokens (including your prompt + its own generated answer so far), it will truncate the oldest parts (for a chat) or simply not process beyond the limit (for a single prompt). In practice, if you have a *very* long conversation, you need to start summarizing or dropping earlier turns once you approach the limit.\n",
    "\n",
    "To get a feel for counting tokens, here‚Äôs a simple way using an approximate method (since we can‚Äôt easily get the model‚Äôs tokenizer without additional libraries):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char count: 650, approx token count: 162.5\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world!\" * 50  # a repetitive text\n",
    "char_count = len(text)\n",
    "approx_tokens = char_count / 4  # using 1 token ~ 4 chars rule\n",
    "print(f\"Char count: {char_count}, approx token count: {approx_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Try with different text lengths:** Change the multiplier or text content to see how the token estimate grows. For example, try 500 repetitions instead of 50 and see the estimate. Keep in mind this is a rough estimat, but it helps gauge scale.\n",
    "\n",
    "**Exercise 5.2: Using max_tokens to simulate context cut-off.** We did an exercise above listing Python features with small `max_tokens`. Let‚Äôs explicitly observe truncation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Being easy to read and write\n",
      "2. Having a large standard library\n",
      "3. Supporting multiple programming paradigms\n",
      "4. Being open source and having a strong community\n",
      "5. Being cross-platform (runs on\n"
     ]
    }
   ],
   "source": [
    "long_prompt = \"Python is an interpreted, high-level programming language.\" * 20  # repeat to make it long\n",
    "payload = {\n",
    "    \"model\": \"openchat-3.5-0106\",\n",
    "    \"prompt\": long_prompt + \"\\nIn summary, Python is known for:\",\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "res = requests.post(f\"{BASE_URL}/v1/completions\", json=payload).json()\n",
    "summary = res.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we gave a long prompt (by repeating a sentence many times) and then asked for a summary. We limited the answer to 50 tokens. Likely, the model will start listing some known Python features but stop partway due to the token cap (you might see it stop at an incomplete point like ‚Äúeasy syntax, large community, and...‚Äù and then cut off).\n",
    "\n",
    "- **Reflect:** If you needed the full summary, you‚Äôd raise `max_tokens`. If the input itself was too long (say we repeated that sentence 1000 times which might exceed context), the model might ignore the later part of the prompt or not be able to include everything in the summary. Thankfully 8192 tokens is quite a lot (our prompt above is far smaller). But if you did have, say, a 50-page document to summarize, you‚Äôd have to chunk it because it wouldn‚Äôt fit in one go.\n",
    "\n",
    "**Question:** *What happens if you ask the model to produce more tokens than the context window allows? For example, setting `max_tokens` to 10000 with this model.*  \n",
    "*(Hint: The model will stop at its context limit; it physically can‚Äôt output beyond ~8192 minus input length. Also, many APIs will throw an error if you set max_tokens too high or simply ignore it beyond the limit.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusion of Lab**\n",
    "\n",
    "In this lab, you:\n",
    "- Set up and connected to a local LLM service (LM Studio running OpenChat-3.5).\n",
    "- Made both completion and chat requests through a standard API interface.\n",
    "- Used system messages to guide the model‚Äôs style and witnessed multi-turn conversation handling.\n",
    "- Adjusted temperature to see its effect on output randomness vs. determinism.\n",
    "- Played with `max_tokens` to control output length and discussed how the context window is a hard limit for the model‚Äôs memory.\n",
    "\n",
    "These hands-on experiments should give you a solid intuition for how LLMs function under the hood and how we control their behavior. Keep these lessons in mind as we proceed to more advanced prompt engineering tasks. The principles of **prompt design** (clear instructions, providing context, etc.) combined with **parameter tuning** (like we did with temperature and max tokens) will be the key to getting great results from LLMs in your applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompten",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
